{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import time\n",
    "\n",
    "import Schedule_pipeline_functions as SP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First filter and structured representation\n",
    "\n",
    "Here the dataset is loaded and for each sentence we search for the matches according to the patterns \"schedule_noun\" and \"delay_verbs\". The output is the structured representation of the matched sentences and it is saved on a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments processed:  100000\n",
      "Time (min) :  2.1434847752253217\n",
      "\n",
      "Total time (min) :  2.143485426902771\n",
      "Number of matches :  6954\n",
      "\n",
      "Number of comments containing at least one of the keword :  7902\n",
      "Number of unique comments retrieved :  5417\n",
      "\n",
      "Number of matches with \"schedule_noun\" pattern :  3884\n",
      "Number of matches with \"delay_verbs\" pattern :  3070\n"
     ]
    }
   ],
   "source": [
    "# load data in chunks from the sample of 100.000 comments\n",
    "comments_chunks = pd.read_csv('data/sample_of_comments.csv.gz', \n",
    "                                chunksize=10000, compression='gzip')\n",
    "# list of responses (to be transformed in DataFrame)\n",
    "results = []\n",
    "n=0 # comments count\n",
    "n_with_keywords = 0\n",
    "\n",
    "t0 = time.time()\n",
    "t_tot = 0\n",
    "\n",
    "for chunk in comments_chunks:\n",
    "    \n",
    "    n += chunk.shape[0]\n",
    "    \n",
    "    if n%100000 == 0:\n",
    "        print('Comments processed: ', n)\n",
    "        print('Time (min) : ', (time.time() - t0) / 60)\n",
    "        t_tot += time.time() - t0\n",
    "        t0 = time.time()\n",
    "        \n",
    "    # get useful info and drop comments with no text\n",
    "    chunk = chunk[['body', 'comment_author', 'thread_id', 'comment_date', 'c_id']].dropna(subset=['body'])\n",
    "    \n",
    "    # loop over comments\n",
    "    for idx, row in chunk.iterrows():\n",
    "        tx, c_id, comment_author, thread_id, comment_date = row.body, row.c_id, row.comment_author, row.thread_id, row.comment_date\n",
    "        \n",
    "        # has one of the keywords?\n",
    "        n_with_keywords += 0 if (\"schedule\" not in tx and 'spac' not in tx and 'dela' not in tx and 'split' not in tx) else 1\n",
    "        # sent tokenizer for comments\n",
    "        for sent in sent_tokenize(tx):\n",
    "    \n",
    "            # apply first filter\n",
    "            idxs_keywords = SP.first_filter_keywords_syntactic(sent)\n",
    "            # obtain structured representation\n",
    "            representation = SP.Structured_representation(idxs_keywords)\n",
    "            representation = [SP.translate_response(i, c_id, comment_author, thread_id, comment_date) \n",
    "                                                                                      for i in representation]\n",
    "\n",
    "            results.extend(representation)\n",
    "            \n",
    "            \n",
    "results = pd.DataFrame.from_dict(results)\n",
    "\n",
    "# assign an order to the columns of the df\n",
    "columns_ordered = ['sent', 'c_id', 'comment_author', 'thread_id', 'comment_date',\n",
    "                   'pattern_matched', 'text_short', 'negations',\n",
    "                   'amod_subj_xcomp_lemma', 'amod_subj_xcomp_lower', \n",
    "                   'compound_subj_xcomp_lemma', 'compound_subj_xcomp_lower',\n",
    "                   'pos_subj_xcomp_lemma', 'pos_subj_xcomp_lower',\n",
    "                   'subject_xcomp_lemma', 'subject_xcomp_lower',\n",
    "                   'verb_xcomp_lemma', 'verb_xcomp_lower', 'verb_phrase_xcomp', 'verb_tense_xcomp',\n",
    "                   'amod_subj_lemma', 'amod_subj_lower',\n",
    "                   'compound_subj_lemma', 'compound_subj_lower',\n",
    "                   'pos_subj_lemma', 'pos_subj_lower',\n",
    "                   'subject_lemma', 'subject_lower', 'subject_active',\n",
    "                   'verb_lemma', 'verb_lower', 'verb_phrase', 'verb_tense',\n",
    "                   'dobj_amod_lemma', 'dobj_amod_lower',\n",
    "                   'compound_dobj_lemma', 'compound_dobj_lower',\n",
    "                   'pos_dobj_lemma', 'pos_dobj_lower',\n",
    "                   'dobj_lemma', 'dobj_lower']\n",
    "\n",
    "results = results[columns_ordered]\n",
    "results.to_csv(\"results/vaccination_schedule/Structured_representation_schedule.csv\")\n",
    "           \n",
    "print()\n",
    "print(\"Total time (min) : \", t_tot/60)\n",
    "print(\"Number of matches : \", results.shape[0])\n",
    "print()\n",
    "print('Number of comments containing at least one of the keword : ', n_with_keywords)\n",
    "print(\"Number of unique comments retrieved : \", results.groupby(['c_id', 'comment_author', \n",
    "                                                           'thread_id', 'comment_date']).apply(lambda rows: 1).sum())\n",
    "print()\n",
    "patterns_matched = results.pattern_matched.value_counts()\n",
    "print('Number of matches with \"schedule_noun\" pattern : ', patterns_matched['schedule_noun'])\n",
    "print('Number of matches with \"delay_verbs\" pattern : ', patterns_matched['delay_verbs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence matched :  i avoided vaccines and once dear daughter was born we put her on a delayed schedule.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                                 136\n",
       "sent                         i avoided vaccines and once dear daughter was ...\n",
       "c_id                                                               c2540129100\n",
       "comment_author                                                littlelivismomma\n",
       "thread_id                                                            a64086145\n",
       "comment_date                                                        09/11/2016\n",
       "pattern_matched                                                  schedule_noun\n",
       "text_short                                    we put her on a delayed schedule\n",
       "negations                                                                    1\n",
       "amod_subj_xcomp_lemma                                                      NaN\n",
       "amod_subj_xcomp_lower                                                      NaN\n",
       "compound_subj_xcomp_lemma                                                  NaN\n",
       "compound_subj_xcomp_lower                                                  NaN\n",
       "pos_subj_xcomp_lemma                                                       NaN\n",
       "pos_subj_xcomp_lower                                                       NaN\n",
       "subject_xcomp_lemma                                                        NaN\n",
       "subject_xcomp_lower                                                        NaN\n",
       "verb_xcomp_lemma                                                           NaN\n",
       "verb_xcomp_lower                                                           NaN\n",
       "verb_phrase_xcomp                                                          NaN\n",
       "verb_tense_xcomp                                                           NaN\n",
       "amod_subj_lemma                                                            NaN\n",
       "amod_subj_lower                                                            NaN\n",
       "compound_subj_lemma                                                        NaN\n",
       "compound_subj_lower                                                        NaN\n",
       "pos_subj_lemma                                                             NaN\n",
       "pos_subj_lower                                                             NaN\n",
       "subject_lemma                                                               we\n",
       "subject_lower                                                               we\n",
       "subject_active                                                          ACTIVE\n",
       "verb_lemma                                                                 put\n",
       "verb_lower                                                                 put\n",
       "verb_phrase                                                                put\n",
       "verb_tense                                                          PastSimple\n",
       "dobj_amod_lemma                                                          delay\n",
       "dobj_amod_lower                                                        delayed\n",
       "compound_dobj_lemma                                                        NaN\n",
       "compound_dobj_lower                                                        NaN\n",
       "pos_dobj_lemma                                                             NaN\n",
       "pos_dobj_lower                                                             NaN\n",
       "dobj_lemma                                                            schedule\n",
       "dobj_lower                                                            schedule\n",
       "Name: 136, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show an example of structured representation of a match\n",
    "example = results.sample().iloc[0]\n",
    "print('Sentence matched : ', example.sent)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FILTER & CLASSIFIER\n",
    "\n",
    "Now apply the filter to the matched sentences to identify schedule-related comments. The label 'FILTERED_OUT' is assgned to the unrelated matches and the classifier is applied to the related ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of filtered out matches :  2560\n",
      "Number of filtered in matches :  4394\n",
      "Number of matches filtered in \"schedule_noun\" pattern :  2469\n",
      "Number of matches filtered in \"delay_verbs\" pattern :  1925\n",
      "\n",
      "Number of matches labeled as \"recommended\" :  1605\n",
      "Number of matches labeled as \"alternative\" :  2789\n",
      "\n",
      "Number of unique comments filtered in :  3746\n"
     ]
    }
   ],
   "source": [
    "# here the new column \"FILTER\" corresponds to the final classification (+1 and -1) or \"FILTERED_OUT\"\n",
    "results.loc[:, 'FILTER'] = results.apply(lambda row: SP.Filter(row), axis=1)\n",
    "\n",
    "# remove filtered out matches\n",
    "results_filtered_out = results[results.FILTER=='FILTERED_OUT']\n",
    "results = results[results.FILTER!='FILTERED_OUT']\n",
    "\n",
    "n_reg_modif = results.FILTER.value_counts()\n",
    "patterns_matched = results.pattern_matched.value_counts()\n",
    "\n",
    "print('Number of filtered out matches : ', results_filtered_out.shape[0])\n",
    "print('Number of filtered in matches : ', results.shape[0])\n",
    "print('Number of matches filtered in \"schedule_noun\" pattern : ', patterns_matched['schedule_noun'])\n",
    "print('Number of matches filtered in \"delay_verbs\" pattern : ', patterns_matched['delay_verbs'])\n",
    "print()\n",
    "print('Number of matches labeled as \"recommended\" : ', n_reg_modif[1])\n",
    "print('Number of matches labeled as \"alternative\" : ', n_reg_modif[-1])\n",
    "print()\n",
    "print(\"Number of unique comments filtered in : \", results.groupby(['c_id', 'comment_author', \n",
    "                                                           'thread_id', 'comment_date']).apply(lambda rows: 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Classifications\n",
    "\n",
    "Once the schedule-related sentences are classified, we proceed by assigning the label to comments by aggregating sentences. We discard:\n",
    "- comments having more than one sentence with discordant labels\n",
    "- sentences containing matches in past tenses\n",
    "\n",
    "Before doing that, we manually modify the tense of all the verbs whose lemma is \"split\" and identified as \"PastSimple\". The dependency parser of SpaCy always assign this tense, even if it may refer to the present. We choose to consider all these verbs at present tense.\n",
    "\n",
    "Finally, we save the file containing the classified comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHANGING tense to verbs whose lemma is \"split\". Number of matches changed :  46\n",
      "\n",
      "DISCARDING past tenses. Number of matches removed :  1074\n",
      "\n",
      "CHECK COMMENTS WITH DISCORDANT LABELS\n",
      "Number unique comments  2957\n",
      "\n",
      "Number comments with more classes  317\n",
      "Number comments with more classes and homogeneous  212\n",
      "Number comments with more classes and discordant  105\n"
     ]
    }
   ],
   "source": [
    "# change tense of split (it is almost always considered as PastSimple)\n",
    "index_of_comments_split_past = results[(results.verb_lemma=='split')\n",
    "                                      &(results.verb_tense=='PastSimple')\n",
    "                                      &(results.verb_phrase=='split')].index\n",
    "\n",
    "results.loc[index_of_comments_split_past, 'verb_tense'] = 'PresentSimple'\n",
    "print('CHANGING tense to verbs whose lemma is \"split\". Number of matches changed : ', len(index_of_comments_split_past))\n",
    "print()\n",
    "\n",
    "# discard past tenses\n",
    "tense_not_to_take = ['PastSimple', 'PastPassive', 'PastContinuous', 'PastPerfect']\n",
    "\n",
    "n_results = results.shape[0]\n",
    "results = results[(~results.verb_tense.isin(tense_not_to_take))\n",
    "                 &(~results.verb_tense_xcomp.isin(tense_not_to_take))]\n",
    "print('DISCARDING past tenses. Number of matches removed : ', n_results-results.shape[0])\n",
    "print()\n",
    "\n",
    "# check how many comments have discordant labels\n",
    "n_with_more_class = 0\n",
    "n_with_same_class = 0\n",
    "n_classes = []\n",
    "\n",
    "for idx, rows in results.groupby(['c_id', 'comment_author', 'thread_id', 'comment_date']):\n",
    "    \n",
    "    n_classes.append(rows.shape[0])\n",
    "    \n",
    "    if rows.shape[0] > 1:\n",
    "        n_with_more_class+=1\n",
    "        \n",
    "        if len(set(rows.FILTER.values))==1:\n",
    "            n_with_same_class+=1\n",
    "            \n",
    "print('CHECK COMMENTS WITH DISCORDANT LABELS')\n",
    "print('Number unique comments ', len(n_classes))\n",
    "print()\n",
    "print('Number comments with more classes ', n_with_more_class)\n",
    "print('Number comments with more classes and homogeneous ', n_with_same_class)\n",
    "print('Number comments with more classes and discordant ', n_with_more_class-n_with_same_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarded comments with discordant labels\n",
      "Final number of schedule-related comments :  2852\n",
      "Number of comments coded as \"recommended\" :  1059\n",
      "Number of comments coded as \"alternative\" :  1793\n"
     ]
    }
   ],
   "source": [
    "# discard comments with discordant labels and assign class for each comment\n",
    "comments_classified = results.groupby(['c_id', 'comment_author', 'thread_id', 'comment_date']).filter(lambda rows:\n",
    "                                                                      len(set(rows.FILTER.values))==1)\n",
    "\n",
    "comments_classified = comments_classified.groupby(['c_id', 'comment_author', \n",
    "                                                   'thread_id', 'comment_date']).apply(lambda rows: \n",
    "                                            pd.Series({'CLASS':list(set(rows.FILTER.values))[0]})).reset_index()\n",
    "\n",
    "print('Discarded comments with discordant labels')\n",
    "print('Final number of schedule-related comments : ', comments_classified.shape[0])\n",
    "n_reg_modif = comments_classified.CLASS.value_counts()\n",
    "print('Number of comments coded as \"recommended\" : ', n_reg_modif[1])\n",
    "print('Number of comments coded as \"alternative\" : ', n_reg_modif[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_id</th>\n",
       "      <th>comment_author</th>\n",
       "      <th>thread_id</th>\n",
       "      <th>comment_date</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c2000186919</td>\n",
       "      <td>Virgo&amp;SparklerMomma</td>\n",
       "      <td>a87195</td>\n",
       "      <td>03/21/2008</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c2000274000</td>\n",
       "      <td>mlbryant_7</td>\n",
       "      <td>a124485</td>\n",
       "      <td>04/04/2008</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c2000380950</td>\n",
       "      <td>pineaple35</td>\n",
       "      <td>a167455</td>\n",
       "      <td>04/16/2008</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c2000840633</td>\n",
       "      <td>Conshusmama</td>\n",
       "      <td>a363035</td>\n",
       "      <td>05/24/2008</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c2000841389</td>\n",
       "      <td>~domestic&amp;tattooed~</td>\n",
       "      <td>a363035</td>\n",
       "      <td>05/24/2008</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          c_id       comment_author thread_id comment_date  CLASS\n",
       "0  c2000186919  Virgo&SparklerMomma    a87195   03/21/2008      1\n",
       "1  c2000274000           mlbryant_7   a124485   04/04/2008      1\n",
       "2  c2000380950           pineaple35   a167455   04/16/2008     -1\n",
       "3  c2000840633          Conshusmama   a363035   05/24/2008     -1\n",
       "4  c2000841389  ~domestic&tattooed~   a363035   05/24/2008      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_classified.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_classified.to_csv(\"results/vaccination_schedule/comments_classified.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipelines_environment",
   "language": "python",
   "name": "pipelines_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
